# データマイニング概観

## 概要
データの山を採掘することで「データの中に埋め込まれている(有用な)知識を発掘」する。本資料では用語の定義や関連事例・技術を概観しつつ、発掘する際の問題意識を眺める。また本授業で扱う部分を示す。

---

### 今日の達成目標
- データセットからパターンを見つける際の問題意識に触れる。（理解は後日で良い）
- 本授業で扱う範囲を確認し、履修判断のために用いる。

---

## 背景
データから有益な知識を発掘したいという問題意識は古くからあるが、コンピュータが広まる前はそもそもデータの蓄積自体が困難だったり、蓄積できるようになってきても十分な計算機パワーがなかった。例えば、2000年頃はまだスマートフォンもなく、個々人が動画を撮影してSNSで共有する、、、といったことはなく、そういう個人用途のデータはまだまだデータとして集まっていなかった。これに対し今では、様々なデータが蓄積され続け、そのスピードは増す一方である。

参考: [Data Never Sleeps 7.0, 2019](https://www.domo.com/learn/infographic/data-never-sleeps-7) by DOMO

---

### データとは

>**[データとは？]**
>
>[Wikipedia: data](https://en.wikipedia.org/wiki/Data)によると以下の通り。
>
>``Data are characteristics or information, usually numerical, that are collected through observation.[1] In a more technical sense, data are a set of values of qualitative or quantitative variables about one or more persons or objects, while a datum (singular of data) is a single value of a single variable.[2]``

考察対象から観測できる情報を **{index}`データ<でーた-データ>`（{index}`data`）** と呼ぶ。現場におけるデータは、質的にも量的にも変数化・値化されていなかったり、どこかに一箇所（DB等）にまとまっていないことも少なくない。整理されていないデータのことを **{index}`生データ<なまでーた-生データ>`（{index}`raw data`）** と呼ぶことにしよう。授業の前半（Part I, II）では、基本的には「データ（既に質的もしくは量的に変数化されているもの）」を対象とする。後半（Part III以降）では主に自然言語を例として生データの処理を扱う。

```{note}
**{index}`質的データ<しつてきでーた-質的データ>`** とは、数値では表せない、観測された特性や属性に関するデータを指します。例えば以下のようなデータを指します。
- カテゴリ：ニュース記事のジャンルに「最新、政治、スポーツ、芸能」といったカテゴリ名。色（赤、青、緑）や感情（怒り、幸せ）もカテゴリの一種です。
- ラベル：「学籍番号、マイナンバー」のようなユニークなインデックス情報。これも他特別することができるという点で質的データです。
- 順序のある値：「1位、2位、3位」、「1年、2年」のように値の絶対値そのものに意味があるというよりも、順序の方により大きな意味がある場合には、これらも質的データとして扱います。
    - NOTE: 逆に言えば、これらはの値そのものが重要であると考える場合には、量的データとして扱います。このように数値を含むデータをどちらで扱うかは利用者次第で位置づけることになります。

**{index}`量的データ<りょうてきでーた-量的データ>`** とは、原則として数値で表される測定可能（＝定量的に扱いやすい）な情報を指します。例えば以下のようなデータを指します。
- 年齢。ただし「子供、青年、成人」のようにカテゴリ分けして扱うこともあります。これは他も同様です。
- 収入。
- 温度。
```

---

### データマイニングとは
一言で言えば、大量のデータの中にある **頻出パターンや意味のある構造を発見** することである。とはいえ知ってる知識が得られるだけでは嬉しくない。新しく、再利用可能な知識（useful new knowledge）を得たい。これを別の言葉で言い換えると例えば以下のようなタスク例となる。
- 「殆どの人間が当たり前にできること」をコンピュータにさせたい。
    - 画像に写っているのは動物か否か？（識別タスク）
    - ある事件について報道している記事はどれか？（分類タスク、クラスタリング、トピック抽出）
- 「場合によっては専門家でも難しいタスク」をコンピュータにさせたい。
    - 売上向上のために取り組むべきアクションは何か？（相関ルール、クラスタリング、推薦、理由説明）
    - 新しい化学反応は無いだろうか？, [化学分野におけるLLMの革命！外部ツールを活用した統合エンジン「ChemCrow」](https://ai-scholar.tech/articles/large-language-models/chemcrow)（生成タスク）
    - 放射線科医が行っていた医療画像からの読影をサポートできないか？, [画像診断と機械学習そしてDeep Learning ReconstructionによるMRI画像にあたえるインパクト](https://www.innervision.co.jp/sp/ad/suite/canonmedical/seminarreport/garon2018_mri)（キャンセル可能性推測、短時間撮影、ノイズ低減、ポジショニング最適化、、）
    - リモートからの不正なアクセスを見抜けないか？（識別タスク、外れ値検出）

---

### 演習
- 「殆どの人間が当たり前にできること」の例を挙げよ。
- 「場合によっては専門家でも難しいタスク」の例を挙げよ。

---

## 関連技術と問題意識

### データベース
> **[データベースとは？]**
>
> [Wikipedia: データベース](https://ja.wikipedia.org/wiki/データベース)によると、、
>
> データベース（英: database, DB）とは、検索や蓄積が容易にできるよう整理された情報の集まり。 通常はコンピュータによって実現されたものを指す。コンピュータを使用したデータベース・システムでは、データベース管理用のソフトウェアであるデータベース管理システムを使用する場合も多い。プログラムで扱うデータ構造やデータそのものを独自実装の場合に比べて少ない工数で操作できるようにしている。

**{index}`データベース<でーたべーす-データベース>`** を利用することで大規模に対する高速な処理を実現できるだけでなく、共通アクセス方法の提供、データ共有、バックアップといった多様な運用・保守管理上のメリットがある。

これに対して本授業ではデータベースは触らず、CSV/TSV/[ARFF](https://waikato.github.io/weka-wiki/formats_and_processing/arff_stable/)等のテキストファイルとしてデータを用意し、それを読み込んで利用することを想定する。

```{tip}
**{index}`CSV`** や **{index}`TSV`** は、表形式でデータを保存するためのテキストファイルの一種です。値をカンマ区切りで保存しているファイルを「CSVファイル」、値をタブ区切りで保存しているファイルを「TSVファイル」と呼び、それぞれ「ファイル名.csv」や「ファイル名.tsv」という拡張子をつけて保存していることが多いです。

- 参考: [Wikipedia: Tab-Separated Values](https://ja.wikipedia.org/wiki/Tab-Separated_Values)
```

### データウェアハウス 1.0 〜 2.0
繰り返しになりますが、一般的にデータベースは様々な目的で検索や蓄積が容易となるように整理された情報の集まりです。容易となるように整理することで効率よく情報を蓄積したり検索したりしやすくしていますが、「ありとあらゆる状況に対して効率よく整理する」ということは現実的ではありません。このため「特定の目的Aに合致するように整理する」ことになります。そしてこのことを言い換えると「当初の目的Aとは異なる目的Bのために利用しようとすると非効率となることがある」ことを意味しています。ここでいう効率や非効率という用語は、目的に即した利用法なら瞬時に結果を得られる（例えば0.01秒）が、非効率な使い方をすると極めて応答が遅くなる（例えば1時間）ということを指します。

任意の状況において効率よく合致するDBはないため、利用目的に即してDBを構築して使い分けることになります。利用目的を大別すると「注文や在庫更新といったリアルタイム処理が主目的のケース」と「大量の履歴データに対して多次元的なレポートや分析が主目的のケース」として捉えることが多いです。それぞれの特徴は以下の通りです。

- 注文や在庫更新といったリアルタイム処理が主目的のケース（**単一DB向き**）
	- 更新頻度が高く、最新の一貫性のあるデータを瞬時に記録したいことが多い。運用システムや基幹システムとも呼ばれる。最新データに対する即時性を最重視するため「過去データを定期的にバックアップ装置に移行して擬似的に削除（参照が求められたときだけ復元する）」ように対処することもある。
- 大量の履歴データに対して多次元的なレポートや分析が主目的のケース（**DWH向き**）
	- 週報、日報、時報のようにある程度まとまった単位でのデータに対する分析であり、即時更新は重視しないことが多い。

このうちデータウェアハウスは後者の分析を目的としています。複数のDBを跨いで分析することが多く、それらのDBをどのように統合し直すかは分析目的を踏まえて検討することになります。このように分析目的に構築した **{index}`データウェアハウス<でーたうえぁはうす>`** が元々の概念です。便宜上本資料ではこれを「データウェアハウス 1.0」と呼ぶことにします。おおよそDBが利用され始めた1970年代時点ではまだ単一DBの利用に留まることが多く、1980年代〜1990年代頃にかけてデータウェアハウス1.0による多次元分析が行われ始めました。

```{note}
データウェアハウス1.0, 2.0, 3.0のような呼称は一般的には用いられていません。當間が便宜上付けた呼称です。
```

2000年代後半頃からは主にWebから得られる半構造化データ（HTML, XML等）や非構造化データ（テキスト、画像等）の爆発的な増加に伴い、分散フレームワーク（Hadoop, Spark, MapReduce等）によるビッグデータ処理が普及し始めました。単にデータの量(volume)が増えるというだけではなく、多様なデータ(variety)を効率よく処理するためのフレームワークです。これらの分散フレームワークを基幹システムとして利用することが増えてきました。そこから少し経ち、2010年代頃からは分析や機械学習も組み込こまれ、Google BigQuery (2010年頃), Amazon Redshift (2012年頃) のような「クラウド型データウェアハウス」が提供され始めました。この2000年代後半〜2010年代のWebデータやクラウド型の時代を便宜上「データウェアハウス 2.0」と呼ぶことにします。

### データウェアハウス 3.0?
今では当たり前になっている生成AIのはしりであるChatGPTは2022年11月30日に公開されました。当時でも自然らしい応答ができるということで全世界に大きな衝撃を与えた日でした。そしてそれとほぼ同時期に[LangChain](https://ja.wikipedia.org/wiki/LangChain)と呼ばれる フレームワークも登場しました。LangChainはLLMを使ったアプリケーション作成をサポートしており、文書解析やコード解析といった「テキストを中心とした非構造化データ」も対象としている点が大きな特徴です。ただし良くも悪くも <u>現時点では発展途上段階のため、数週間前に動いたアプリが最新版では動かなくなるような破壊的更新も少なくない点には注意が必要</u>です。

生成AI（LLMやVLM）をベースにした分析や処理ではこれまでと異なる点があります。

- **事前学習済みモデルを利用する。**
	- データウェアハウス2.0までは収集したデータだけで処理していました。これに対し生成AI時代ではモデル自体が大量のデータで事前学習済みであるためモデル自体が「知識ベース」になっており、処理に組み込みやすくなりました。これは極めて大きなアドバンテージとなっていますが、一方で動作をコントロールしにくく、倫理や安全面も踏まえて大きな問題になっています。
- **非構造化データを対象にしやすい。**
	- 特にテキストならばほぼ問題なく入力できる。一般的にDBではどのように表形式に構造化するかが肝でしたが、生成AI時代では文章や画像から必要な情報をモデル自身が抽出して把握する力が格段に強くなりました。これに伴い一般ユーザのPC内ドキュメントを対象として処理するフレームワークも生まれてきています。

この2つの特徴を有するデータウェアハウスを便宜上「データウェアハウス 3.0」と呼ぶことにします。必ずしも大規模なデータを対象としてはいませんが、「常識」のような概念を踏まえたモデルを利用し、非構造化データを効率よく処理できるようになってきました。これらの特徴を踏まえた応用事例を紹介します。

- ソフトウェア開発 x AI
	- 開発支援AIとしては Copilot が早くから提供されていましたが、これはあくまでも主体はプログラマ本人という立ち位置でした。これに対し2024年夏頃から提供され始めた[Cline](https://cline.bot/), [OpenHands](https://docs.all-hands.dev/)では「人間は指示をするナビ役」という位置づけで具体的な作業をAIに任せるという立ち位置まで入り込んできました。AIは指示を受けて計画を立て、（許可を与えれば）実行するところまでやります。
	- 参考
		- [AIエージェント Cline とともに開発してみて感じた衝撃と感想「使わにゃ損」](https://qiita.com/itTkm/items/306d5587220e146187e2), 2025年1月29日
		- [MCPで広がるLLM　〜Clineでの動作原理〜](https://zenn.dev/codeciao/articles/cline-mcp-server-overview), 2025年3月8日
- ログ化 x AI
	- LLM登場以降、様々なデータを「ベクトルをキーとして保存するベクトル・データベース」として扱うことが急速に広まってきました。これを利用して様々な作業ログ自体をデータとして保存し、必要に応じて後から再利用するということもやりやすくなってきました。
	- 参考
		- [Claude Desktopとmcp-server-qdrantで超お手軽ナレッジベースの構築](https://zenn.dev/inurun/articles/fc0ec63cad574b), 2025年3月25日

---

### 教務情報を例にデータ種別を眺めてみよう
例えば琉大では[教務情報システム](http://rais.skr.u-ryukyu.ac.jp/dc/)にログインすると、学生は自身の過去の成績を参照したり、現在履修中の講義やシラバス、お知らせを確認することができる。これらは代表的な構造化データであり、表形式で表されるレコードやトランザクションの例である。成績は例えば以下のように構成されている。

| (レコード識別子) | 科目番号 | 科目 | 単位 | 評価 | 修得年度 |
| ---: | --- | --- | --- | --- | --- |
| 1 |工共114|プログラミングI|2|F|2019年度 前期|
| 2 |工共114|プログラミングI|2|C|2020年度 前期|
| 3 |工共214|プログラミング演習I|2|B|2020年度 前期|
| 4 |知能984|データマイニング|2|A|2021年度 前期|

「特定個人の成績」という観点からは、各データは「レコード識別子・科目番号・科目・単位・評価・修得年度の6つの **{index}`属性<ぞくせい-属性>`（{index}`attribute`）** 」で構成される **{index}`レコード<れこーど-レコード>`（{index}`record`）** の集合として表現される。

上記ではレコード識別子1〜4の4件のレコードが列挙されている。レコード識別子はシステムの都合上付けることが多いものであり、例えば「プログラミングIを複数回受講しているがこれらを別データとして区別して保存したい」場合には各データを区別して記録する必要がある。これを区別（識別）するためにユニークな値を用意したものをレコード識別子と呼ぶ。

また、識別子以外の主要な属性がアイテムである時、例えば「2020年度 前期」に修得した科目名「プログラミングI, プログラミング演習I」という集合を **{index}`トランザクション<とらんざくしょん-トランザクション>`（{index}`transaction`）** と呼ぶ。別例を上げると、コンビニで各商品に付けられている価格やメーカー名等がレコードであり、ある顧客が一度の会計処理で購入した商品群（1件でも良い）がトランザクションである。レコードやトランザクションは[データベース](https://ja.wikipedia.org/wiki/データベース)における最も基礎となる概念である。

レコードは複数の属性で表される一つのデータのことであり、各属性は **{index}`カテゴリカル属性<かてごりかるぞくせい-カテゴリカル属性>`（{index}`categorical`）・{index}`量的属性<りょうてきぞくせい-量的属性>`（{index}`quantitative`）・{index}`数値型属性<すうちがたぞくせい-数値型属性>`（{index}`numerical`）** に分けられる。更にカテゴリカル属性は、単に互いを区別できれば良い **{index}`名義属性<めいぎぞくせい-名義属性>`（{index}`nominal`）** と、**{index}`順序関係が成り立つ属性<じゅんじょかんけい-順序関係が成り立つ属性>`（{index}`ordinal`）** とに分けられる。データの捉え方は他にもあり、例えば数値型属性を **実数（連続値）と自然数（離散値）** とで区分する考え方や、割合や濃淡といった **濃度** で区分することもある。

なお、属性のことを機械学習の分野では **{index}`特徴<とくちょう-特徴>`もしくは{index}`特徴量<とくちょうりょう-特徴量>`（{index}`feature`）** と呼ぶことが多い。本授業では特徴と呼ぶこととする。同様にレコードのことを **{index}`サンプル<さんぷる-サンプル>`（{index}`sample`）** と呼ぶこととする。

```{tip}
これらの用語は統計学・機械学習・データマイニングといった分野毎に異なる呼び方をすることも少なくない。以下に呼称例を示す。本授業においてはなるべく機械学習呼称を用いるが、他の呼称にも慣れておくことを推奨する。

| データマイニング | 機械学習 | 統計学 |
| --- | --- | --- |
| 属性(attribute) | 特徴(feature) | 独立変数（説明変数; explanatory variable） |
| 属性集合？(用語なし) | 特徴ベクトル(feature vector) | 多変量変数(multivariate data) |
| 目的変数？ | 教師データ(supervisory signal)、目的変数(target)、クラス(class)、ラベル(label) | 目的変数(response variable) |
| レコード(data, record) | サンプル(sample), 標本(example) | サンプル(sample),標本(example) |
```

---

### 演習
[2021年4月の那覇の気象データ](https://www.data.jma.go.jp/obd/stats/etrn/view/daily_s1.php?prec_no=91&block_no=47936&year=2021&month=4&day=&view=p1)を参考に、以下の問いに答えよ。
- 質的データの例を挙げよ。
- 量的データの例を挙げよ。
- 過去1週間の最高気温を参考にし、翌日の最高気温を予測したいとしよう。このとき統計学でいうところの説明変数や目的変数は何に相当するか？
- 説明変数や目的変数は、機械学習ではどのように呼ぶだろうか？

---

### 卒業要件を例に複数数値データを眺めてみよう
知能情報コースにおける卒業要件を例に取ると、各科目は「共通教育、専門基礎科目、専門科目」に大別される。仮に各項目における評価をGPAにより求めるとするならば、各学生を3つの評価軸で構成されたベクトル空間上にプロットすることができるだろう。

より一般的には、データセットを構成するレコードが複数の数値データを含む場合、そのレコードは **{index}`多次元空間<たじげんくうかん-多次元空間>`** 上の点としてみなすことができる。属性数が3種類までなら3次元空間として描画可能だが、4属性以上になると直接的な描画は困難となる。このような場合には属性数そのものを取捨選択する[特徴選択](https://ja.wikipedia.org/wiki/特徴選択)や、主成分分析に代表される[次元削減](https://en.wikipedia.org/wiki/Dimensionality_reduction)により属性数を絞り込むことがよく行われる。

---

### 血液検査を例に考えてみよう
[日本予防医学協会の説明](https://www.jpm1960.org/exam/exam06.html)に検査項目ごとの基準値が示されている。例えばASTは0〜40U/L、総コレステロールは130〜219mg/dL、HbA1cは4.6〜5.5%といった具合だ。数値で表されている項目に限定すると、すべての項目が数値で表されているため、多次元空間上の点としてプロット可能である。実際、特徴選択や次元削減等を通してプロットすることで貧血や肝臓の異常といった病気傾向に属する人を見つけやすくなるだろう。

その一方で、属性ごとの値の意味や高低の基準は不揃いである。このため「HbA1cが低い人」と言われてもそれがどのぐらいの値を指すのかは各基準を参照しないとわからない。そのため[日本予防医学協会の説明](https://www.jpm1960.org/exam/exam06.html)には項目ごとに基準値が示されている。例えば総コレステロールが1ポイント上がった場合と、HbA1cが1ポイント上がった場合とでは全く意味合いが異なる。このように数量的に意味合いや度合いが異なる特徴量を上手く取り扱うための前処理は **{index}`スケーリング<すけーりんぐ-スケーリング>`（{index}`scaling`）** と呼ばれており、特に **{index}`正規化<せいきか-正規化>`（{index}`normalization`）** や **{index}`標準化<ひょうじゅんか-標準化>`（{index}`standalization`）** が良く用いられる。

スケーリングの効果は[ワインの分類](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html)の例を眺めてみるとイメージしやすいだろう。[ワインデータセット](https://archive.ics.uci.edu/ml/datasets/wine)は同じ地域で栽培されたぶどうから作った異なる3種類のワインがあり、それを識別するためにアルコール度数・アミノ酸・フェノール等13種の特徴量が用意されている。スケーリング前のデータで分類学習すると約81%の精度となり、それをPCAで2次元描画したものが左図である。これに対しスケーリング後の精度は約98%であり、右図の通り直感的にも分類しやすい場所に各サンプルがうまく散らばっていることを目視できる。

---

### カテゴリカル属性のデータはどのように扱えば良いのだろう？
数値データであれば、[ユークリッド距離](https://ja.wikipedia.org/wiki/ユークリッド距離)等を用いることで距離を計測することができる。距離がわかるなら、各サンプルがどれだけ近いのか（≒似ているのか）を判断することができる。<u>距離（距離空間）を利用したいからこそ、特徴ベクトルを数値データで記述している</u>。

カテゴリデータの場合、例えばニュース記事のジャンルとして「政治」と「スポーツ」があったとき、それらの距離はどう測ればよいだろうか？　データセットとして「1つ目の記事（サンプル）は政治に関する記事である」「2つ目の記事はスポーツに関する記事である」という関連情報を何とかして機械的に処理しやすい数値に置き換えて処理したい。このとき、例えば政治を1、スポーツを2として単純に数値を割り当てて問題はないのだろうか。言い換えると「政治とスポーツの距離は1」という演算結果は妥当だろうか。現実的には **One-Hot表現（One-Hot encoding）** によりベクトル表現することが多い。

---

### 自然言語で記述された文章はどのように扱えばよいのだろう？
自然言語（私たちが普段使っている言葉）をコンピュータに理解させるための処理。ここでいう理解とは、特定タスクの達成に寄与すること全般を指す。例えば、ある映画レビューがポジティブに評価しているのか、ネガティブに評価しているのかを文章から判断することができれば、このタスクにおいては理解しているとみなすとしよう。タスクの例としては以下のようなものがある。

- 記事分類: [自動記事分類技術を用いた「日経テレコンナビ型記事検索」　情報収集に気づきを提供する新スタイル](https://www.jstage.jst.go.jp/article/johokanri/57/12/57_900/_article/-char/ja/)
    - 「利用者が思いついたキーワードを起点として、検索された結果を複数の観点で自動的に分類し、利用者が求めている情報（検索結果）にたどり着くことができるようになっている。」
- レビュー分析: [絵本レビューを情報源とする子どもの認知発達的反応の収集・類型化とそれに基づく絵本の分類](https://www.jstage.jst.go.jp/article/jsoft/30/3/30_581/_article/-char/ja/)
    - 「本論文で情報源とする絵本レビューサイトにおいては、実際に絵本の読み聞かせを行った養育者や保育者が、絵本の読み聞かせを行った際の様子や絵本への感想、絵本に対する子供の反応の有無やその反応の様子など、多岐多様な情報を記載している。」
- 極性判定: [Twitterを対象とした評価極性判定システムの構築](https://www.jstage.jst.go.jp/article/pjsai/JSAI2013/0/JSAI2013_1F41in/_article/-char/ja/)
    - 「（中略）ある対象に関する評価を含むツイートを収集し、評価対象・属性・評価後の3つ組を抽出し、ツイートの評価極性を判定することによりその実現を試みる。」
- クラスタリング: [テキストマイニングによる食レポ口コミからのユーザー嗜好モデリング](https://akita-pu.repo.nii.ac.jp/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=1015&item_no=1&page_id=13&block_id=21)
    - 「（中略）もし、和食と洋食で季節感を感じさせる単語が一致し、似たような分析結果が出れば、以前、和食の飲食店を利用したユーザーに同じ旬の食材を使った料理や似たような季節性の高い食べ物を提供する洋食の飲食店も提案することができるだろう。」
- 固有名認識、関係抽出: [生命情報学が直面する大規模ゲノムデータ時代の課題 : 3.生命科学分野におけるテキストマイニング](https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=67186&item_no=1&page_id=13&block_id=8)
    - 「多くの研究者がさまざまな実験を行い、その結果を次々と論文で発表しているため、得られた知見は主に構造化されていない自然言語の形で集積されてゆく。研究の進展や領域の細分化に伴い、論文が発表される頻度も高くなっている。このような状況において、特定の研究課題に関連する文献を効率的に見つけ出し、上記のような情報を取得してゲノム情報と紐付けた形でのデータベースを構築するためにテキストマイニング技術を利用することが多くなっている。ただし対象となるデータの規模と種類は広く、1つの研究室レベルですべてに対応できないため、テキストマイニング技術を用いたシステムの公開や必要なリソースの共有が行われている。本稿では生命科学分野におけるテキスト処理技術の現状、BioHackathon 2009 で議論された事項を踏まえて解説する。」

これらを実現するためには、文章を **シソーラス・カウント・推論等** により処理しやすい表現へ置き換えて処理することが多い。推論ベースの代表的な手法には **word2vec, fastText, BERT** 等の分散表現がある。

```{tip}
単語や文章の分散表現（もしくは埋め込み(embeddings)）として表現する技術は、ここ数年でマルチモーダルをも対象として研究が盛んに行われている。例えば、2015年に[VQA (Visual Question Answering)](https://visualqa.org)と呼ばれるタスクが提案された。VQAとは、ある画像とその画像に関する質問を与えられた時に正しい答えを返すことが求められるタスクである。このタスクに取り組むため、画像を表現するベクトルと質問を表現するベクトルをそれぞれ直列に並べただけのシンプルな形を用いて取り組んだ。このシンプルな方法で2015年当時は約54%であったが、[2021年5月時点では約80%](https://visualqa.org/roe.html)に迫っている。
```

---

### グラフは？
表構造では傾向把握が困難なデータの一つにグラフがある。ここでいうグラフとは、ノード集合と辺集合があり、辺がノードの対で表現された構造を指す。辺で向きを考慮する場合には有向グラフ、考慮しない場合には無向グラフと呼ばれる。例えばSNSにおけるユーザのフォロー関係やウェブページのリンク関係をグラフとして表現することができる。

![ego-Facebook](./figs/facebook_combined_s.jpg)

上記は約4千ノード、8.8万エッジからなる[Social circles: Facebook](http://snap.stanford.edu/data/ego-Facebook.html)を描画した図である。フォロワー数が多いという点で重要な人物を探し出すぐらいならフォロワー数だけで判断できるが、それ以上の「グラフ構造」を考慮した傾向分析は困難である。例えば、コミュニティを検出したり、情報がどのように伝達されたのか、繋がりがどのように発展してきたのか、はたまた今後はどのように発展するだろうかといった事柄を見積もるためにはグラフとして捉え、パターンを抽出する必要がある。

グラフは文章よりは構造化されているが、パターンの幅が多岐にわたるため有益知識の発掘は一筋縄ではいかない。これらは数値化されていないという意味でどちらも **{index}`半構造化データ<はんこうぞうかでーた-半構造化データ>`** である。半構造化データの場合には、まず何を情報として捉え、それをどのように処理しやすいデータに置き換えるかを検討する必要がある。

---

### データセットとしてよくある問題1：不均衡データ
これまではデータを構成する要素、すなわち特徴の観点からその特性や問題点を概観してきた。これらとは違い、サンプル集合としてのよくある問題に「**{index}`不均衡<ふきんこう-不均衡>`**」がある。

「欲しい事例は、そうでない事例と比べると集めるコストが高くなりがち」のため、結果的にデータセットにおけるデータ数は偏りが大きくなるであることが多い。例えば以下で示している「うつ傾向」を細分化して予測する場合、低うつ傾向と高うつ傾向とでは等しい割合でサンプルが揃っているとは限らない。不均衡データの場合、マイニング手法が優れていたとしても適切なパターンを得ることは困難になりがちである。

- [うつと関連性が低いとされている生活習慣等からのうつ傾向予測](https://www.jstage.jst.go.jp/article/pjsai/JSAI2019/0/JSAI2019_2P5J201/_article/-char/ja/)
    - 「（中略）そこで本稿では、まず、どの生活習慣、気質、体質がうつ状態と関連していないとみなされているかを明らかにし、それらの生活習慣等の中で、実際には関係しているものを予測するモデルを利用し特定することを目的とした。この時、実際のデータに近いうつ傾向高い人が少ない不均衡なデータで試みた。」
    - 表1: L2正則化ロジスティック回帰の混同行列
      - ![表1: L2正則化ロジスティック回帰の混同行列](./figs/jsai2019_2p5j201-table1.png)
- [深層学習抽出特徴量から生成した擬似特徴量を用いた不均衡データ多クラス画像分類](https://www.jstage.jst.go.jp/article/pjsai/JSAI2018/0/JSAI2018_2A104/_article/-char/ja/)
    - 「データの中に少量しかないクラスがあると、そのクラスの識別率が極端に低下してしまう場合がある。そのようなデータは現実のデータ分析では頻繁に遭遇する。（中略）」

---

### データセットとしてよくある問題2：バイアスを含むサンプル
[UK police are using AI to inform custodial decisions](https://www.wired.co.uk/article/police-ai-uk-durham-hart-checkpoint-algorithm-edit)によると、イギリスでは容疑者が2年間で再犯を犯すリスクがどの程度かを見積もるシステムHART（Harm Assessment Risk Tool）を導入している。あくまでも判断材料の一つとしてリスクの高さを提示するシステムである。初期のアルゴリズムでは再犯リスクに関与すると思われる特徴として年齢・性別・居住区域（郵便番号）等、34種類のデータを用いていた。しかしながら本アルゴリズムは「ある居住区域に住む人々は再犯率が高い」ということを根拠として、ある特定個人の再犯率が高いだろうと予測してしまう、公平とは呼べない提示を指定してしまうことがあった。元々は判定材料に関係するであろうと用意した特徴であったが、それをどう用いるかという点では不適切な材料になっていた。

このようにデータセット構築時の特徴が、利用目的に照らし合わせるとそもそも不適切であったというケースはよくある。また特徴が妥当だとしても収集したサンプル自体に偏りがあったり、データそのものに誤りが含まれていることもある。このような視点をデータセットの公平性として捉える研究も出始めている。

---

## ケーススタディ（特徴量の大切さを確認しよう）

### Shapes problem
- 出典: Shapes problem by p.76, [Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition](http://www.cs.waikato.ac.nz/ml/weka/book.html)

下図 Figure 3.9 のような図形が並んでいる。各図形が「立っている(standing)」のか、「そうではない(unstanding)」のかを識別するタスク（**{index}`二値分類タスク<にちぶんるいたすく-二値分類タスク>`** と呼ばれる）を考えてみよう。

![FIGURE 3.9 The shapes problem: shaded = standing; unshaded = lying.](./figs/book-practical-machine-learning-figure3.9.png)

ここでは、standing, unstanding を識別するための重要な特徴として、図形の横の長さ、縦の長さ、辺の数の3つを用意したとする。各サンプルについて整理したデータが Table 3.2 だ。

![Table 3.2 Training Data for the Shapes Problem](./figs/book-practical-machine-learning-table3.2.png)

このデータセットににおける頻出パターンマイニングの結果、例えば「if height >= 3.5 then standing」が得られるかもしれない。これは有益なパターンだろうか？

このような「嬉しくないルール」を獲得する理由は、頻出パターンマッチングの場合には属性間の関係を全く考慮しないためである。学習手法によっては考慮することも可能だが、高コストのため全パターンを考慮することは避けることが多く、独立していることを想定しているモデルも少なくない（e.g., 線形回帰モデル）。例えば属性数Nのデータセットにおいて、全属性間の2要素間関係を考慮するとどれだけのコストが必要になるだろうか？（それを3要素、4要素、、、と増やしていった場合、どこまで増えるだろうか？それは現実的だろうか？）

---

### 特徴量工学！
以上のように、目的に沿った特徴を用意することと、またその特徴からどのようにパターンを抽出するのかを適切に選ぶことが機械学習やデータマイニングにおいては重要である。特に前者の、特定タスクに特化して機能する特徴量を検討し利用することを **{index}`特徴量工学<とくちょうりょうこうがく-特徴量工学>`（{index}`feature engineering`）** 呼ぶ。本授業では、タスク特化の特徴量も扱うが、汎用性の高い扱い方（＝各種データの取り扱い）についても取り上げる。なお教員の興味の問題で、タスク特化特徴量については自然言語の例示が多く、静止画・音声・動画像あたりのデータはほとんど取り扱わない予定である。

- サマリ
  - 用意したデータが何なのか理解せず、獲得したいルールを想定せず、どのようなモデルを利用しているかを把握せずに機械学習等でパターンを抽出しようとすると、類似状況に直面していることに気づかないことがある。不適切なパターン（ルール）を利用しても効果的な結果は得られないため、これらの理解が大切。

```{tip}
授業の狙いを振り返るために、解説記事「怪奇!! 次元の呪い - 識別問題、パターン認識、データマイニングの初心者のために」の[前編](https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=64233&item_no=1&page_id=13&block_id=8)を読んでみよう。
```

---

### データからパターンを発掘する大まかな流れ
- 生データ -> 特徴量表現 -> 処理
    - 生データ：実世界にある何かしら
    - 特徴量：生データを実数値ベクトル等、機械が処理しやすい形式に落とし込んだもの
        - 観測値 vs 専門家等人手のかかる値
        - 簡単にわかるデータから、手間のかかるデータを得たい。
    - 機械学習：特徴量から概念への写像
        - 「猫とはどのような特徴を持つか？」を数学的に記述するのは困難。このような概念をデータセットから自動で学ばせたい。

---

## 授業で触れる範囲、触れない範囲

### 授業で扱う範囲
- 知識発掘時によくある問題。
    - 特徴量工学：各種データ（数値・カテゴリ・文章）の取り扱い。
      - （余裕があれば：グラフ、時系列データなども）
    - 不均衡データ。
    - 文章特有の問題。
      - 単語、新語・未知語、概念的上位下位関係、代名詞、、、
- いくつかの代表的なマイニング技術（機械学習、統計的手法）。
- これらの実装例、事例紹介。
    - ある程度の実装サポートも。
    - 「ライブラリの使い方」 vs 「その裏側で動いている仕組み」
    - 事例紹介
      - 論文における主張の妥当さも重要だが、それ以上にアプローチの仕方が面白そうに感じるもの（當間主観）を紹介。
- [No Free Lunch](https://ja.wikipedia.org/wiki/ノーフリーランチ定理)
    - 授業でこの内容に触れるというよりは、これを前提として様々な考え方・アプローチを紹介する。
- 関連分野との位置づけ
  - 人工知能（の一部に機械学習）：知能らしさに対する工学的アプローチ。どちらかというと「らしさを実現すること」に重きを置くケースが多い。
    - 機械学習の立場：小さなパターンなら手動で書くこともできるが、例えば「画像処理における猫らしさ」をパターンとして書き下すのは極めて困難。そのようなパターン生成を自動化しようと試みているが、そのパターンは人間の基準と同一とは限らない。
  - 統計学：データ分析アプローチ。どちらかというと「何故こうなるのかを説明すること」に重きを置くケースが多い。
  - [OR（Operations Resarch）](https://ja.wikipedia.org/wiki/オペレーションズ・リサーチ)：最適化、意思決定理論。ある工場において、どのスケジュールで素材を手配し、組み立て、配送するのが最も効率良くなるだろうか、というように「分析対象をシステムとして定式化し、その最適解や近似解を効率良く求める」ことに重きを置く。
  - データマイニングでは「これらを道具としてバランス良く使い、有用な知識発掘をする」ことを目指す。

---

### 授業で触れない箇所
- 知能情報コースで提供している他科目の内容
    - [データサイエンス](https://tiglon.jim.u-ryukyu.ac.jp/portal/Public/Syllabus/SyllabusSearchStart.aspx?lct_year=2024&lct_cd=617008001)の基礎
    - [人工知能](https://tiglon.jim.u-ryukyu.ac.jp/portal/Public/Syllabus/SyllabusSearchStart.aspx?lct_year=2024&lct_cd=617013002)の基礎
    - [機械学習](https://tiglon.jim.u-ryukyu.ac.jp/portal/Public/Syllabus/SyllabusSearchStart.aspx?lct_year=2023&lct_cd=617027001)の基礎
      - 「機械学習」については選択科目でもあるため、概要ぐらいは触れる予定。ただし、本授業だけでは十分な時間が取れない（重複を避ける）ため、注意すること。
      - e.g., モデル、回帰・分類・クラスタリング、特徴ベクトル、学習データ、テストデータ、交差検証、LOO、混同行列、、、
- 下記も同様の理由、もしくは授業目標から外れる内容が多いため、本授業では扱いません。
    - 分散システム
    - 並列計算
    - データベース
    - リアルタイム処理
    - 高速化等実用的な実装
    - [MLOps](https://ja.wikipedia.org/wiki/MLOps)
      - MLOps失敗事例: [実はAIを使っている、長崎ちゃんぽん「リンガーハット」　予測精度が突如ダウン……どう立ち向かったのか？](https://www.itmedia.co.jp/business/articles/2203/10/news007.html)

---

(prepare)=
## 次回に向けた予習
機械学習について概説している以下のドキュメントを読み、自分なりに気になる事柄を整理せよ。疑問等は、次回授業の前日までに、別途用意するフォームに入力すること。

- 例1: [An introduction to machine learning with scikit-learn](https://scikit-learn.org/1.4/tutorial/basic/tutorial.html)の冒頭「Machine learning: the problem setting」から「Learning and predicting」あたりまで。
    - scikit-learn 1.4 で古いと書かれていますが、概要を理解するためのドキュメントとしては問題ありません。
- 例2: [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course)
    - 自動翻訳されて「機械学習集中講座」になってる模様。「クラッシュコースを開始する」→「線形回帰（70分）」あたりまで。
- Python/Numpy/Pandas/Matplotlib初心者の人
    - Python
        - [Progate](https://prog-8.com/), オンライン演習しながら学べます。
        - [Python Tutorial](https://docs.python.org/3/tutorial/), 公式チュートリアル
        - [プログラミング1](https://ie.u-ryukyu.ac.jp/~tnal/2024/prog1/static/Readme.html), 知能情報コース2024年度授業資料。
        - + Numpy: [Python Numpy チュートリアル](https://avinton.com/academy/python-numpy-tutorial-japanese/)
    - Numpy
        - [Numpy Quickstart](https://numpy.org/doc/stable/user/quickstart.html)
    - Pandas
        - [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html)
    - Matplotlib
        - [tutorial](https://matplotlib.org/stable/tutorials/index.html)
