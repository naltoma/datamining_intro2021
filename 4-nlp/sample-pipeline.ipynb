{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024年 6月21日 金曜日 15時30分29秒 JST\n",
      "Python 3.9.6\n"
     ]
    }
   ],
   "source": [
    "!date\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformers.pipelineを用いた推定例\n",
    "参考\n",
    "- [公式ドキュメントのクイックツアー](https://huggingface.co/docs/transformers/v4.41.3/ja/quicktour)\n",
    "\n",
    "基本的には ``pipeline(task=\"タスク名\")`` としてタスクを指定するだけでそれに応じたモデルをダウンロードして利用することができる。しかしモデルは必ずしも日本語には対応していないため、場合によっては想定以上に推定結果が悪くなることもある。このノートブックでは「感情分析（極性推定）」を例にデフォルトのモデルを利用した場合の失敗例を示しつつ、それ以降は「適切に動きそうなモデルを探し、指定して利用する」形での実行例を示す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感情分析（極性推定）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 失敗例\n",
    "タスクを指定し、モデルを指定しない場合にはデフォルトで想定されているモデルが自動でダウンロードされる。\n",
    "この例では \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\" が自動で選ばれている。\n",
    "モデル名から分かるとおり英語で学習したモデルのため、日本語だとうまく推定できない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.889319896697998}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# これはモデルマッチしていないため、うまく推定できない例\n",
    "pipe = pipeline(task=\"sentiment-analysis\")\n",
    "text = \"この映画はとても素晴らしい\"\n",
    "outputs = pipe(text)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルを指定した例\n",
    "https://huggingface.co/models からタグ「Text classification」を選び、\"sentiment jp\" ぐらいのキーワードで絞り込むと5件がヒットする。この中から[mr4/bert-base-jp-sentiment-analysis](https://huggingface.co/mr4/bert-base-jp-sentiment-analysis)を選んだ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.9999532699584961}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"mr4/bert-base-jp-sentiment-analysis\"\n",
    "pipe = pipeline(task=\"sentiment-analysis\", model=model_name)\n",
    "text = \"この映画はとても素晴らしい\"\n",
    "outputs = pipe(text)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.999911904335022}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 別例\n",
    "text = \"正直わかりずらい。むだに間があるし。\"\n",
    "outputs = pipe(text)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全ラベルに対するスコア\n",
    "オプションで \"return_all_scores=True\" を付けることで、全てのラベルに対するスコアを取得できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tnal/.venv/opencalm/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'label': 'negative', 'score': 0.999911904335022},\n",
       "  {'label': 'positive', 'score': 8.806267578620464e-05}]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 別例\n",
    "text = \"正直わかりずらい。むだに間があるし。\"\n",
    "outputs = pipe(text, return_all_scores=True)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキスト生成\n",
    "タグ「Text Generation」を選び、\"jp\" で絞り込み、「Most downloads」でソート。その中では小さめのモデル（1.3Bならメモリ1GBぐらいあれば動作する）である[llm-jp/llm-jp-1.3b-v1.0](https://huggingface.co/llm-jp/llm-jp-1.3b-v1.0)を選択。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tnal/.venv/opencalm/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/Users/tnal/.venv/opencalm/lib/python3.9/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'ようやく晴れてきましたね。どこかに出かけたい気分です。 さて、今日'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"llm-jp/llm-jp-1.3b-v1.0\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model_name)\n",
    "text = \"ようやく晴れてきましたね。どこかに\"\n",
    "outputs = pipe(text)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 質問応答\n",
    "### 失敗例（BERT系）\n",
    "タグ「Question Answering」を選び、\"jp\"で絞り込んだ中から1番目のモデル[jpabbuehl/distilbert-base-uncased-finetuned-squad]を選びました。BERT系は比較的小さなモデルですが、その分あまり精度は良くないかもしれません。（失敗している）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 5.8764489949680865e-05,\n",
       " 'start': 0,\n",
       " 'end': 24,\n",
       " 'answer': '病気の都合で車を運転することができなくなりました'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"jpabbuehl/distilbert-base-uncased-finetuned-squad\"\n",
    "pipe = pipeline(task=\"question-answering\", model=model_name)\n",
    "question = \"先生はどこに住んでいますか?\"\n",
    "context = \"病気の都合で車を運転することができなくなりました。徒歩で通勤できると楽になるため、大学の近くに住んでいます。\"\n",
    "outputs = pipe(question=question, context=context)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 少し大きめなtext-generationモデルで回答させてみる\n",
    "text-generationモデルは入力の与え方を工夫することで質問応答モデルとしても利用できる。ここでは　text-generation で用意した \"llm-jp/llm-jp-1.3b-v1.0\" を用いて質問文を用意してみた。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'question: 先生はどこに住んでいますか?\\ncontext: 病気の都合で車を運転することができなくなりました。徒歩で通勤できると楽になるため、大学の近くに住んでいます。\\nanswer: 大学の近くに住んでいるので、大学の近くに住んでいると思います。\\n\\n'}]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"llm-jp/llm-jp-1.3b-v1.0\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model_name, max_length=100)\n",
    "question = \"先生はどこに住んでいますか?\"\n",
    "context = \"病気の都合で車を運転することができなくなりました。徒歩で通勤できると楽になるため、大学の近くに住んでいます。\"\n",
    "inputs = f\"question: {question}\\ncontext: {context}\\nanswer: \"\n",
    "outputs = pipe(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: 先生はどこに住んでいますか?\n",
      "context: 病気の都合で車を運転することができなくなりました。徒歩で通勤できると楽になるため、大学の近くに住んでいます。\n",
      "answer: \n"
     ]
    }
   ],
   "source": [
    "# 用意した入力文の中身\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "質問: 先生はどこに住んでいますか?\n",
      "関連情報: 病気の都合で車を運転することができなくなりました。徒歩で通勤できると楽になるため、大学の近くに住んでいます。\n",
      "回答: \n",
      "---\n",
      "[{'generated_text': '質問: 先生はどこに住んでいますか?\\n関連情報: 病気の都合で車を運転することができなくなりました。徒歩で通勤できると楽になるため、大学の近くに住んでいます。\\n回答: 大学の近くに住んでいるのであれば、大学の近くの病院に行ってください。\\n\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# 別の与え方\n",
    "question = \"先生はどこに住んでいますか?\"\n",
    "context = \"病気の都合で車を運転することができなくなりました。徒歩で通勤できると楽になるため、大学の近くに住んでいます。\"\n",
    "inputs = f\"質問: {question}\\n関連情報: {context}\\n回答: \"\n",
    "print(inputs)\n",
    "print(\"---\")\n",
    "outputs = pipe(inputs)\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencalm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
