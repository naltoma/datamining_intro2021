# 課題3：自然言語処理してみよう
- 全体の流れ
  - 自身で用意したコーパスを元に、文書毎の単語分布や特徴的な単語等を観察することで文書群の傾向を掴みたい。なお Level 4 ではカテゴリカルなメタデータを用いてカテゴリ間比較することを想定している。この点を踏まえ、自身で比較観察したい文書群を用意しよう。
    - 例えば「沖縄県の施策における特徴的な点を掴みたい」と考えているなら、沖縄県の取り組みに関する資料群間での比較よりは、沖縄を含む他都道府県との比較をしたほうが効果的である可能性が高い。
  - Level 1: コーパスを用意し、pd.DataFrameとして読み込もう。
  - Level 2: 文書を眺めてみよう1（分かち書き、固有表現抽出、類似文書検索）。
  - Level 3: ベクトル化してみよう。
  - Level 4: 文書を眺めてみよう2（単語分布2種類）。
  - オプション例

<hr>

## Level 1: コーパスを用意し、pd.DataFrameとして読み込もう。
- 注意
  - コーパスの著作権に注意しよう。
    - 原則として、コーパスはそれを用意している組織や各記事の作者らが権利を有する。勉強を含む個人用途のために利用する分には構わないが、他の人に対してデータを再配布する等の行為は行わないように。
- コーパスの条件
  - 複数文書あること。文書数20件以上であること。総文字数4000字以上であること。
    - あまりにも大規模なコーパスである場合には、条件を満足する範疇で間引いても良い。
    - 「文書」とは処理対象の単位を指している。
      - 例えば[授業評価アンケートの自由記述欄を収集したデータ](./load_r_assesment.ipynb)では個々人の1回答文を1文書として扱っている。1回当分の中に複数文が含まれることもあれば、短文であることもあるが、文の数や単語数を問わず「1回答文=1文書」として扱っている。このためこのデータセット例は複数文書あるものとして判断して良い。
      - 別の例として[Wikipedia:自然言語処理](https://ja.wikipedia.org/wiki/自然言語処理)は長文である。しかしこのページに記載されている全文を1文書として扱うならば、これは1文書であるために「複数文書」という条件を満足していない。1ページ=1文書として扱うならば、複数ページに渡って文書を集める必要があることになる。
  - 教師データとしカテゴリカルなメタデータ（ラベルでも良い）があること。
    - 手動で用意しても良い。
    - 例えば[授業評価アンケートの自由記述欄を収集したデータ](./load_r_assesment.ipynb)においては、各コメントに対して授業名、学年、必修選択といったカテゴリカルなメタデータがある。
  - 自身で検討できない人向けの案
    - [livedoor ニュースコーパス](https://www.rondhuit.com/download.html#news%20corpus)
    - [青空文庫](https://www.aozora.gr.jp)
      - 例えば分野名や作家名がメタデータになりうる。
      - 参考: [Pythonで青空文庫データを自然言語処理向けにさくっと一括テキスト整形＋前処理](https://qiita.com/dzbt_dzbt/items/593dbd698a07c12a771c)
    - 都道府県庁、市町村等の自治体を対象とし、教育に対する施策を集める。
    - 進路として興味のある企業らの募集要項、理念、待遇などを集める。
- DataFrameの条件
  - 処理対象となるテキストを保存している列名を text とすること。
- レポートに含める内容
  - コーパスの説明
    - 概要（100〜200字程度）
    - コーパスの取得手順
    - pd.DataFrame.head()の出力結果。
  - Level 1 で利用したプログラムのファイル名。
    - レポート中にはファイル名を記載しよう。別途プログラムファイルをアップロードすること。``level1.py``などのように、ファイル名から類推しやすい名称を付けると良い。
    - もしコードで工夫した点があれば、レポートで説明(PR)すること。

<hr>

## Level 2: 文書を眺めてみよう1。
- (1) 分かち書きし、総単語数（総token数）を求めよ。句読点等の記号を含めて良い。
- (2) 固有表現抽出を行い、気になる固有表現を2件選べ。その2件がどのように使われているのか文例を各々探し出し、分かったことを述べよ。
- Level 2 で利用したプログラムファイル。
  - レポート中にはファイル名を記載しよう。別途プログラムファイルをアップロードすること。``level2.py``などのように、ファイル名から類推しやすい名称を付けると良い。
  - (1),(2)で別々のファイルでも良い。
  - もしコードで工夫した点があれば、レポートで説明(PR)すること。

<hr>

## Level 3: ベクトル化してみよう。
- (3) (2)で選んだ固有表現2件ならびに全文書についてBoW, TFIDF, word2vecいずれかでベクトル化し、最大類似文書上位3件を類似度とともに示せ。また類似文書について、類似度合いがどのぐらい妥当か主観で評価せよ。
- (4) ベクトル化した文書について、カテゴリ分類タスクを実行せよ。
  - 5分割交差検定しその平均精度を示すこと。
  - 5分割交差検定で最高精度となったモデルによる混同行列を示すこと。
  - 混同行列を観察し分かることを考察すること。
- Level 3 で利用したプログラムファイル。
  - レポート中にはファイル名を記載しよう。別途プログラムファイルをアップロードすること。``level3.py``などのように、ファイル名から類推しやすい名称を付けると良い。
  - (3),(4)で別々のファイルでも良い。
  - もしコードで工夫した点があれば、レポートで説明(PR)すること。

<hr>

## Level 4: 文書を眺めてみよう2。
- 背景
  - ここではカテゴリ別の傾向を観察していきたい。実際にデータを観察する前に何かしら仮説を立て、その仮設が妥当かどうかを確認するという立場で観察することを強く勧める。
  - なお (5),(6) の単語は頻出単語上位N件を想定しているが、仮説確認のためにはそれ以外の単語セットが適していると思われる場合には変更して良い。ただし設定基準を説明すること。
- (5) カテゴリ別に単語の出現回数をカウントし、積み上げ棒グラフにより描画せよ。グラフから分かることを述べよ。
- (6) 任意の2カテゴリを選び、scattertextにより単語出現分布を可視化せよ。(3)で述べたこと以外で、scattertextから分かることを述べよ。
- Level 4 で利用したプログラムファイル。
  - レポート中にはファイル名を記載しよう。別途プログラムファイルをアップロードすること。``level4.py``などのように、ファイル名から類推しやすい名称を付けると良い。
  - (5),(6)で別々のファイルでも良い。
  - もしコードで工夫した点があれば、レポートで説明(PR)すること。

<hr>

## オプション例
- ストップワードを設定してみよう。
- 固有表現を拡張してみよう。
  - 参考: [NLP | GINZA v5で固有表現抽出のルール追加を試してみた](https://note.com/lizefield/n/n18fcac42afea)
- 目的語やその係り受け先の単語を抽出し、どのような使われ方をしているのか観察してみよう。
- Transformersの事前学習済みモデルを用いて分類タスクを学習してみよう。
