# 課題4： JNLIタスクに挑戦
今回は実験結果まで準備してあります。各自でコードを確認して実行したい場合には[report4_tips](./report4_tips.md)を参照してください。

## 背景
LLMの開発が進むことで「システムに対してテキストで指示する」形式でのやりとりが増えてきました。しかしながらLLMは万能というわけではなく、「全く根拠のない回答文を生成する」「一部の条件を見落とす」「関係ない情報を勝手に追加する」等のような想定外の挙動をしてしまうことも少なくありません。このような問題へ体系的に取り組むために「対象としているシステムが苦手なタスクは何か」といったことを理解しやすくするために様々なベンチマークを構築し改善に結びつける試みが行われています。

その一例として、今回の課題では日本語言語理解ベンチマーク（Japanese General Language Understanding Evaluation, JGLUE）を取り上げます。
- 関連サイト
    - 論文: [JGLUE: 日本語言語理解ベンチマーク](https://www.jstage.jst.go.jp/article/jnlp/30/1/30_63/_pdf)
    - 著者による解説記事: [日本語言語理解ベンチマークJGLUEの構築 〜 自然言語処理モデルの評価用データセットを公開しました](https://techblog.yahoo.co.jp/entry/2022122030379907/)
    - Github: [yahoojapan/JGLUE](https://github.com/yahoojapan/JGLUE/)

---
## JNLIタスク
JGLUEには複数のタスクがあります。今回はその中から JNLI (Japanese Natural Language Inference, 自然言語推論)タスクに取り組んで貰います。

NLIとは、前提文（premise sentence）と仮説文（hypothesis sentence）の文ペアが与えられたときに、前提文が仮説文に対して持つ推論関係を「含意（entailment）」「矛盾（contradiction）」「中立（neutral）」の3値で識別する分類タスクです。[GitHubで公開されているタスクガイドライン](https://github.com/yahoojapan/JGLUE/blob/main/task_guidelines.md#jnli-step-1-inference-relation-judgement)から例を取り上げると、以下のようなタスクです。

```{admonition} JNLIタスクのイメージ
2つの文A, Bが与えられますので、
文Aの光景を想像してください。

想像した文Aの光景において、文Bの内容が成立し得るかを
以下の3つの選択肢から選んで回答してください。

・「Aである」ならば確実に「Bである」
・「Aである」ならば「Bであるかもしれない」
・「Aである」ならば確実に「Bではない」

以下の例をよく読んで回答してください。

例１
A：黒い犬が鳴いています。
B：犬が鳴いています。
答え：「Aである」ならば確実に「Bである」
理由：Aの内容から確実に犬が鳴いていると判断できるため。
```

以下ではAを前提文、Bを仮説文と呼称します。NLIタスクにおける入出力は以下の通りです。
- 入力1: 前提文
- 入力2: 仮説文
- 出力: 「含意（entailment）」「矛盾（contradiction）」「中立（neutral）」の3ラベルで回答

---
## 3手法概要
NLIタスクに向けて3手法（BoW, word2vec, BERT）による分類モデルを構築します。

### BoW(TF-IDF)方式
- (1) 前提文と仮説文をそれぞれ分かち書きする。
- (2) 前提文と仮説文を「前提文[SEP]仮説文」として1つの文に結合する。ここまでの処理は preprocessing.ipynb で実行済み。
    - `[SEP]` は前提文と仮説文とを区別するために追加した separation (分割) を表すトークンです。
- (3) 結合した文集合を[TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)でベクトル化。
- (4) シンプルな分類モデル[LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)で学習。

### word2vec方式
- (1) 前提文、仮説文をトークンに分け、それぞれの分散ベクトルを取得。
- (2) 前提文の平均ベクトル、仮説文の平均ベクトルを求め、これらを結合したベクトルを構築する。これを特徴ベクトルとする。ここまでの処理は preprocessing.ipynb で実行済み。
- (3) [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)で学習。

### BERT方式
- (1) 前提文と仮説文を「前提文[SEP]仮説文」として1つの文に結合する。
    - コード上は tokenizer に2つの文を入力しているだけ。参考: [文のペアの前処理](https://note.com/npaka/n/n36acd2122192#GcjJi)
- (2) 結合した文をトークン系列に分割し、paddingして長さを揃える。
- (3) PyTorch tensor 形式に変換。
    - トークンID系列（input_ids）
    - アテンションマスク（attention_mask）
    - 教師ラベル（labels）
- (4) BERTでファインチューニング（微調整）。

---
## Level 1: 検討課題1: JNLIタスクへの理解を深めよう （約30分想定）
次の文ペア①〜③を読み、仮説文が前提文から **必ず** 言える（entailmentに該当する）かどうかをあなた自身で判断しなさい。

| 例 | 文①（前提文）            | 文②（仮説文）        |
| - | ------------------ | -------------- |
| ① | 「男の子が水たまりで遊んでいる。」  | 「子どもが遊んでいる。」   |
| ② | 「女性が赤い傘をさして歩いている。」 | 「雨が降っている。」 |
| ③ | 「列車が駅に到着した。」       | 「列車が完全に停止した。」  |

レポートには以下の事項について報告しなさい。
- (1) ①〜③のうち、2文目が“必ず”言えるのはどれか？
- (2) (1)でそう考えた理由を、以下に示すヒントを参考に述べよ。（50字以上〜200字以内）

＜(2)へのヒント＞

| 手がかりのイメージ        | 具体的に見るポイント               | 誤判定につながる例                                   |
| ---------------- | ------------------------ | ------------------------------------------- |
| **言いかえ・同じ意味の単語** | 「男の子」⇔「子ども」、「車」⇔「自動車」    | 同じ単語が無いと“別物”と思って neutral にしやすい              |
| **反対の意味**        | 「ある」⇔「ない」、「開く」⇔「閉じる」     | 反対語を知らないと entailment と contradiction を取り違える |
| **否定を示す語**       | 「ない」「しない」「〜ではない」         | 否定語を無視すると真逆に分類する                            |
| **数量・範囲の言い方**    | 「すべて」「ひとつも」「いくつか」        | 「いくつか」を「すべて」と誤解 → 誤判定                       |
| **言い切り／推測**      | 「到着した」(完了) と「止まった」(別の状態) | 状態変化を誤解すると neutral か contradiction          |

---
## Level 2: 検討課題2: アプローチ毎の特性を把握しよう （約1.5時間想定）
本課題では前述した *JNLIタスク* に取り組む。データは自然言語で入力されることを踏まえ、分類モデルを (a) Bag-of-Words方式、(b) word2vec方式、(c) BERT方式の3つのアプローチで構築することを考える。それぞれのアプローチ概要は `3手法概要` で述べたとおりである。

これらを踏まえ、3つのアプローチそれぞれについて (1) 手法上の特徴を説明し、(2)その特徴がモデルの予測性能に与える影響について考察せよ。
- 備考
    - (1)の説明は、それぞれ50字以上200字以内とする。
    - (2)の考察は、それぞれ100字以上500字以内とする。
- ヒント
    - 説明はアプローチ概要および専門用語が持つ特性を踏まえて記述すると良い。
    - 考察は、この時点では実験結果を観察する必要はない。あくまでも「このアプローチだからこうなるだろう」ということを論理立てて説明することを求めている。
    そのアプローチを取ることでどういうケースだとうまくいきそうか、もしくは失敗しそうなケースは何か、、といった成否を踏まえて検討すると良いだろう。

---
## Level 3: 3手法の比較 （約2時間想定）
3手法の実行結果を見比べ、(1)〜(4)に取り組め。

- 実行結果
    - BoW(TF-IDF)方式の実行結果: [bow.ipynb](./bow.ipynb)
    - word2vec方式の実行結果: [word2vec.ipynb](./word2vec.ipynb)
    - BERT方式の実行結果: [bert.ipynb](./bert.ipynb)

- (1) 分類タスクという観点からはaccuracy(精度)だけで評価しても十分良さそうである。しかし本コードでは詳細分類レポート（accuracy, precision, recall, f1-score）、更には混同行列等の結果も示している。このように accuracy 以外の評価項目を列挙している理由は何だろうか。100字以上400字以内で述べよ。
    - ヒント
        - 分類レポートと混同行列の読み方は[classification_report, ConfusionMatrixDisplayの実行例](../../2-ml-intro/classification_evaluations.ipynb)を参照しよう。
        - 精度（accuracy）だけでは不十分な場面を考えてみよう。
- (2) BoWモデルで最も誤分類した割合が多かったラベルはどれか？
    - レポートには以下を報告せよ。
        - (a) 誤分類が多かったラベル名
        - (b) 該当ラベルにおける失敗事例1件（複数でも良い）
        - (c) 提示した失敗事例を誤分類した要因（50字以上200字以内）
    - ヒント
        - (c)の要因は level 1 と重複する部分が多くても構いません。ただしできるだけ事例に基づいた考察をしてください。
- (3) BERTで最も誤分類した割合が多かったラベルとその要因は何か？
    - レポートには以下を報告せよ。
        - (a) ラベル名
        - (b) 当該ラベルにおける失敗事例1件（複数でも良い）
        - (c) 提示した失敗事例を誤分類した要因（50字以上200字以内）
- (4) 全体として、どのモデルが最も性能が高いといえるだろうか？
    - レポートには以下を報告せよ。
        - (a) モデル名
        - (b) 判断した根拠
